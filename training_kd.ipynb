{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "YrLAHZiW_utf"
      },
      "source": [
        "修改DIR跟model_save_location變數，把檔案放進DIR\n",
        "\n",
        "wav跟txt放在同一個目錄(DIR)\n",
        "\n",
        "檔名要一樣，副檔名.wav and .txt\n",
        "\n",
        "可直接全部執行"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ubxjWxiH_YS-"
      },
      "outputs": [],
      "source": [
        "DIR = 'kick_drum'\n",
        "LABELS = ['no', 'yes'] # Has onset: no = 0, yes = 1\n",
        "NUM_CLASSES = len(LABELS)\n",
        "PLOT_SAVE_LOCATION = '.'\n",
        "model_save_location = './model_kd'\n",
        "\n",
        "CHANNELS = [2048]  # [1024, 2048, 4096]\n",
        "MEL_BANDS = 80\n",
        "TIME_FRAMES = 12\n",
        "DIFF_FROM_ONSET_MS = 0.03\n",
        "THRESHOLD_FREQ = 15000\n",
        "\n",
        "BATCHES = [64]#, 256, 512]\n",
        "EPOCHS = 50\n",
        "PATIENCE = 150\n",
        "TRAIN_TEST_SPLIT = 0.10\n",
        "TRAIN_VAL_SPLIT = 0.20\n",
        "\n",
        "# Categorical cross-entropy expects labels to be provided in a one-hot representation (0, 1).\n",
        "LOSS_FUNCTION = 'categorical_crossentropy'\n",
        "LEARNING_RATE = 0.001\n",
        "\n",
        "INPUT_SHAPE = (MEL_BANDS, TIME_FRAMES, len(CHANNELS))\n",
        "PRED_LAYER_ACTIVATION = 'sigmoid'\n",
        "METRICS = ['acc']\n",
        "PREC_REC_FSCORE_AVERAGE = 'macro'  # None # 'weighted' # 'micro'\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "OwKUKXKy_iAs"
      },
      "source": [
        "# Some functions for processing data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "8DA3DxrWo_Gp"
      },
      "outputs": [],
      "source": [
        "from processing import SpectrogramProcessor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "sx_i52UiA_Js"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-06-22 22:32:05.101652: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import os.path\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "from pathlib import Path\n",
        "\n",
        "import librosa\n",
        "import librosa.display\n",
        "from pydub import AudioSegment\n",
        "from scipy.io import wavfile\n",
        "from scipy.signal import stft, spectrogram\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "\n",
        "class DatasetGenerator:\n",
        "    def __init__(self, \n",
        "                 label_set, \n",
        "                 sample_rate=44100,\n",
        "                 channels=[2048],\n",
        "                 mel_bands=80,\n",
        "                 time_frames=15,\n",
        "                 diff_from_onset_ms=0.030,\n",
        "                 threshold_freq=15500):\n",
        "        \n",
        "        self.label_set = label_set\n",
        "        self.sample_rate = sample_rate\n",
        "        self.channels = channels\n",
        "        self.mel_bands = mel_bands\n",
        "        self.time_frames = time_frames\n",
        "        self.diff_from_onset_ms = diff_from_onset_ms\n",
        "        self.threshold_freq = threshold_freq    \n",
        "\n",
        "    def construct_wav_annotation_pair_data_frame(self, directory):\n",
        "      files = list(Path(dir).rglob('*wav'))\n",
        "      wav_annotation_pairs = []\n",
        "\n",
        "      for file in files:\n",
        "        wav_file_path = os.path.join(dir, file.name)\n",
        "        filename_base = Path(wav_file_path).stem\n",
        "        annotation_file_path = os.path.join(directory, filename_base + '.txt')\n",
        "\n",
        "        if os.path.isfile(annotation_file_path):\n",
        "                wav_annotation_pair = (wav_file_path, annotation_file_path)\n",
        "                wav_annotation_pairs.append(wav_annotation_pair)\n",
        "      data_frame = pd.DataFrame(wav_annotation_pairs, columns=['wave_file', 'annotation'])\n",
        "      self.data_frame = data_frame\n",
        "\n",
        "      return data_frame\n",
        "\n",
        "\n",
        "    def read_wav_file(self, x):\n",
        "        # Read wavfile using scipy wavfile.read.\n",
        "        _, wav = wavfile.read(x) \n",
        "        \n",
        "        # Normalize.\n",
        "        wav = wav.astype(np.float32) / np.iinfo(np.int16).max\n",
        "        \n",
        "        wav_dim = np.shape(wav)\n",
        "        if len(wav_dim) == 2:\n",
        "            # Convert stereo to mono.\n",
        "            wav = wav.sum(axis=1) / 2\n",
        "        \n",
        "        return wav\n",
        "\n",
        "\n",
        "    def process_wav_file(self, wav_file, annotation_file, win_length=2048, eps=1e-10):\n",
        "      wav = self.read_wav_file(wav_file)\n",
        "      sample_rate = self.sample_rate\n",
        "      hop_length = 441 # win_length // 4 # 2048 // 4 = 512\n",
        "      noverlap = win_length - hop_length#***********************************************************************************************\n",
        "      \n",
        "      freqs, times, spec = spectrogram(wav, sample_rate, window='hann', nperseg=win_length, noverlap=noverlap, mode='complex')\n",
        "      _, S_percussive = librosa.decompose.hpss(spec, margin=(1.0, 5.0))\n",
        "      S = librosa.feature.melspectrogram(S=np.abs(S_percussive), sr=sample_rate, window='hann', win_length=win_length, hop_length=hop_length, n_mels=self.mel_bands, center=False, fmax=self.threshold_freq)\n",
        "      S_db = librosa.core.power_to_db(S, ref=np.max)\n",
        "      S_expanded = np.expand_dims(S_db, axis=2)\n",
        "\n",
        "      spectrograms = []\n",
        "      sp = SpectrogramProcessor(S_expanded, times, annotation_file)\n",
        "      spectrograms = sp.split_spectrogram(self.time_frames)\n",
        "      annotations = sp.get_annotations()\n",
        "      onsets = sp.get_onsets(spectrograms, annotations, self.diff_from_onset_ms)\n",
        "      spectrograms = [s[0] for s in spectrograms]  # Remove time indices.\n",
        "\n",
        "      return spectrograms, onsets\n",
        "\n",
        "    \n",
        "    def split_train_test_set(self, test_size, random_state):\n",
        "      self.df_train, self.df_test = train_test_split(\n",
        "                  self.df, \n",
        "                  test_size=test_size, \n",
        "                  random_state=random_state)\n",
        "      \n",
        "\n",
        "    def apply_train_test_split_by_windows(self, test_size, shuffle_train_data=True):\n",
        "        self.df_train = self.df\n",
        "        data, labels = self.get_train_test_validation_data('train', shuffle_train_data=shuffle_train_data)\n",
        "        \n",
        "        larger_portion = int(len(data)*(1-test_size))\n",
        "        train_data = data[:larger_portion]\n",
        "        train_labels = labels[:larger_portion]\n",
        "        test_data = data[larger_portion:]\n",
        "        test_labels = labels[larger_portion:]\n",
        "\n",
        "        # Remove effects of to_categorical function, convert to binary.\n",
        "        test_labels = np.argmax(test_labels, axis=1)\n",
        "        return train_data, train_labels, test_data, test_labels  \n",
        "\n",
        "\n",
        "    def load_wav_and_annotation_files(self, dir):\n",
        "        files = list(Path(dir).rglob('*wav'))\n",
        "        data = []\n",
        "\n",
        "        # Loop over files to get samples.\n",
        "        for file in files:\n",
        "            wav_file = os.path.join(dir, file.name)\n",
        "            filename_base = Path(wav_file).stem\n",
        "            # Files with '_acc' contain only the accompaniment track.\n",
        "            if '_acc' in filename_base:\n",
        "                continue\n",
        "            else:\n",
        "                # NOTE: Modify the annotation file format and paths to suit your\n",
        "                # needs.\n",
        "                annotations_path = os.path.join(dir, filename_base + '.txt')\n",
        "            # If wav file has matching drum instrument annotation file, add to input data.\n",
        "            if os.path.isfile(annotations_path):\n",
        "                sample = (wav_file, annotations_path)\n",
        "                data.append(sample)\n",
        "\n",
        "        # Data Frames with wavs and matching annotation paths.\n",
        "        df = pd.DataFrame(data, columns=['wav_file', 'annotations'])\n",
        "        self.df = df\n",
        "        return df\n",
        "\n",
        "#-------\n",
        "    def get_train_test_validation_data(self, mode, shuffle_train_data=True):\n",
        "        if mode == 'train':\n",
        "            df = self.df_train\n",
        "            # Shuffle input data.\n",
        "            audiofile_ids = random.sample(range(df.shape[0]), df.shape[0]) if shuffle_train_data else list(range(df.shape[0])) \n",
        "        elif mode == 'val':\n",
        "            df = self.df_val\n",
        "            audiofile_ids = list(range(df.shape[0]))\n",
        "        elif mode == 'test':\n",
        "            df = self.df_test\n",
        "            audiofile_ids = list(range(df.shape[0]))\n",
        "        else:\n",
        "            raise ValueError('The mode should be either train, val or test.')        \n",
        "        return self.get_singlechannel_data(df, audiofile_ids, mode == 'test')\n",
        "\n",
        "#---------\n",
        "    def get_singlechannel_data(self, df, audiofile_ids, is_test):\n",
        "        input_data = []\n",
        "        labels = []\n",
        "\n",
        "        for i in range(0, len(audiofile_ids)):\n",
        "            for win_length in self.channels:\n",
        "                spectrograms, onsets = self.process_wav_file(df.wav_file.values[i], df.annotations.values[i], win_length=win_length)\n",
        "                input_data.extend(spectrograms)\n",
        "                labels.extend(onsets)\n",
        "        \n",
        "        # Convert to numpy array.\n",
        "        input_data = np.array(input_data)\n",
        "\n",
        "        if not is_test:\n",
        "            # Process labels to one-hot encoding.\n",
        "            labels = to_categorical(labels, num_classes=len(self.label_set))\n",
        "\n",
        "        return input_data, labels"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "kT3LfbJGCald"
      },
      "source": [
        "# CNN model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "rIOGCM4xCZjN"
      },
      "outputs": [],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.layers import InputLayer\n",
        "\n",
        "def deep_cnn_sequential(features_shape, num_classes, act='relu'):\n",
        "    \"\"\" CNN model for a single drum instrument training.\n",
        "    May use the same model for e.g. snare, bass drum and hi-hat onset training.\n",
        "    \"\"\"\n",
        "    model = keras.Sequential()\n",
        "\n",
        "    model.add(InputLayer(name='inputs', input_shape=features_shape, dtype='float32'))\n",
        "    # Block 1\n",
        "    model.add(tf.keras.layers.Conv2D(10, (3, 7), activation='relu', padding='same', strides=1, name='block1_conv', input_shape=features_shape))\n",
        "    model.add(tf.keras.layers.MaxPooling2D((3, 1), strides=(2,2), padding='same', name='block1_pool'))\n",
        "    model.add(tf.keras.layers.BatchNormalization(name='block1_norm'))\n",
        "    \n",
        "    # Block 2\n",
        "    model.add(tf.keras.layers.Conv2D(20, (3, 3), activation='relu', padding='same', strides=1, name='block2_conv'))\n",
        "    model.add(tf.keras.layers.MaxPooling2D((3, 1), strides=(2,2), padding='same', name='block2_pool'))\n",
        "    model.add(tf.keras.layers.BatchNormalization(name='block2_norm'))\n",
        "\n",
        "    # Flatten\n",
        "    model.add(tf.keras.layers.Flatten(name='flatten'))\n",
        "    \n",
        "    # Fully connected layer 1\n",
        "    model.add(tf.keras.layers.Dense(256, activation='relu', name='dense'))\n",
        "    model.add(tf.keras.layers.BatchNormalization(name='dense_norm'))\n",
        "    model.add(tf.keras.layers.Dropout(0.5, name='dropout'))\n",
        "    \n",
        "    # Prediction (Fully connected layer 2)\n",
        "    # 2 predictions: onset or no onset\n",
        "    model.add(tf.keras.layers.Dense(num_classes, activation=act, name='pred'))\n",
        "\n",
        "    # Print network summary\n",
        "    model.summary()\n",
        "\n",
        "    return model"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "U21-meInCgH-"
      },
      "source": [
        "# Some functions for training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cdSGSU1a-DCj",
        "outputId": "0373de76-fc24-4fc4-8c9d-f74ebe564a8a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_61444/2078683331.py:56: WavFileWarning: Chunk (non-data) not understood, skipping it.\n",
            "  _, wav = wavfile.read(x)\n",
            "/tmp/ipykernel_61444/2078683331.py:56: WavFileWarning: Chunk (non-data) not understood, skipping it.\n",
            "  _, wav = wavfile.read(x)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training data size:  118910\n",
            "Test data size:  13213\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "\n",
        "import matplotlib\n",
        "from matplotlib import pyplot as plt\n",
        "# Disable showing figures to prevent random GPU failures.\n",
        "matplotlib.use('Agg')\n",
        "\n",
        "import os\n",
        "import random as python_random\n",
        "import codecs\n",
        "\n",
        "#from sklearn.metrics import accuracy_score, confusion_matrix, precision_recall_fscore_support, precision_recall_curve, plot_precision_recall_curve\n",
        "from sklearn.utils import class_weight\n",
        "\n",
        "import tensorflow as tf\n",
        "from keras.callbacks import EarlyStopping\n",
        "\n",
        "def prepare_data(directory):\n",
        "    # Set to global scope for easy access in other functions.\n",
        "    global TRAIN_DATA\n",
        "    global TRAIN_LABELS\n",
        "    global TRAIN_LABELS_1D\n",
        "    global TEST_DATA\n",
        "    global TEST_LABELS\n",
        "\n",
        "    dsGen = DatasetGenerator(label_set=LABELS,\n",
        "                             sample_rate=44100,\n",
        "                             channels=CHANNELS,\n",
        "                             mel_bands=MEL_BANDS,\n",
        "                             time_frames=TIME_FRAMES,\n",
        "                             diff_from_onset_ms=DIFF_FROM_ONSET_MS,\n",
        "                             threshold_freq=THRESHOLD_FREQ)\n",
        "    \n",
        "    dsGen.load_wav_and_annotation_files(directory)\n",
        "    TRAIN_DATA, TRAIN_LABELS, TEST_DATA, TEST_LABELS = dsGen.apply_train_test_split_by_windows(test_size=TRAIN_TEST_SPLIT, shuffle_train_data=True)\n",
        "\n",
        "    print('Training data size: ', len(TRAIN_DATA))\n",
        "    print('Test data size: ', len(TEST_DATA))\n",
        "\n",
        "\n",
        "prepare_data(DIR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "ja5EFUI9CDcD"
      },
      "outputs": [],
      "source": [
        "def plot(metric1, metric2, label1, label2, save_location, id, batch_size):\n",
        "    \"\"\"\n",
        "    Creates and saves the plotted figure.\n",
        "    \"\"\"\n",
        "    try: \n",
        "        fig = plt.figure()\n",
        "        plt.plot(metric1, label=label1)\n",
        "        plt.plot(metric2, label=label2, linestyle='dashed')\n",
        "        plt.legend()\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.grid(linestyle='dotted')\n",
        "        # plt.ylim(top=)\n",
        "        # plt.show()\n",
        "        plt.savefig(save_location + id + '_' + DRUM_INSTRUMENT + '_' + str(EPOCHS) + '_' + str(batch_size) + '.pdf')\n",
        "        plt.clf()\n",
        "        plt.cla()\n",
        "        plt.close(fig=fig)\n",
        "    except Exception as e:\n",
        "        print('Failed to create plot: ', e)\n",
        "\n",
        "\n",
        "def get_model():\n",
        "    \"\"\"\n",
        "    Build and compile a CNN model. \n",
        "    \"\"\"\n",
        "    # Reset scheduled learning rate.\n",
        "    #learning_rate_schedule = CustomScheduleTanh(warmup_steps=3000, phase_step=25000, max_lr=LEARNING_RATE)\n",
        "    learning_rate_schedule=0.001\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate_schedule, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
        "    \n",
        "    model = deep_cnn_sequential(INPUT_SHAPE, NUM_CLASSES, act=PRED_LAYER_ACTIVATION)\n",
        "    model.compile(optimizer=optimizer, loss=LOSS_FUNCTION, metrics=METRICS)\n",
        "    return model\n",
        "\n",
        "\n",
        "def train(model, batch_size):\n",
        "    \"\"\"\n",
        "    Train the model and return history results.\n",
        "    \"\"\"\n",
        "    global TRAIN_LABELS_1D\n",
        "    global TRAIN_DATA\n",
        "    global TRAIN_LABELS\n",
        "\n",
        "    callbacks = [EarlyStopping(monitor='val_loss', min_delta=0.01, patience=PATIENCE, verbose=1, mode='auto')]\n",
        "    # Balance imbalanced onset classes.\n",
        "    #class_weights = class_weight.compute_class_weight('balanced', np.unique(TRAIN_LABELS_1D), TRAIN_LABELS_1D)\n",
        "\n",
        "    history = model.fit(x=TRAIN_DATA, \n",
        "                        y=TRAIN_LABELS, \n",
        "                        batch_size=batch_size,\n",
        "                        epochs=EPOCHS,\n",
        "                        verbose=1,\n",
        "                        callbacks=callbacks,\n",
        "                        validation_split=TRAIN_VAL_SPLIT)\n",
        "    return history\n",
        "\n",
        "\n",
        "def predict(model):\n",
        "    global TEST_DATA\n",
        "    global TEST_LABELS\n",
        "\n",
        "    y_true = TEST_LABELS\n",
        "    #y_pred = model.predict_classes(x=TEST_DATA, verbose=1)\n",
        "    y_pred = model.predict(TEST_DATA, verbose=1) \n",
        "    y_pred = np.argmax(y_pred, axis=1)\n",
        "\n",
        "    return y_true, y_pred\n",
        "\n",
        "\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, precision_recall_fscore_support, precision_recall_curve\n",
        "def get_metrics(y_true, y_pred):\n",
        "    acc_score = accuracy_score(y_true, y_pred)\n",
        "    precision, recall, fscore, _ = precision_recall_fscore_support(y_true, y_pred, average=PREC_REC_FSCORE_AVERAGE)\n",
        "    return precision, recall, fscore, acc_score\n",
        "\n",
        "\n",
        "def get_confusion_matrix(y_true, y_pred):\n",
        "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
        "    return tn, fp, fn, tp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "rJ4IdbNcCEvM"
      },
      "outputs": [],
      "source": [
        "def run(batch_size):\n",
        "  global model_save_location\n",
        "  \n",
        "  start = time.time()\n",
        "  model = get_model()\n",
        "  history = train(model, batch_size)\n",
        "\n",
        "  model.save(model_save_location)\n",
        "\n",
        "  y_true, y_pred = predict(model)\n",
        "  precision, recall, fscore, acc_score = get_metrics(y_true, y_pred)\n",
        "  '''\n",
        "  tn, fp, fn, tp = get_confusion_matrix(y_true, y_pred)\n",
        "  '''\n",
        "  acc = history.history['acc']\n",
        "  val_acc = history.history['val_acc']\n",
        "  loss = history.history['loss']\n",
        "  val_loss = history.history['val_loss']\n",
        "\n",
        "  # Write your own logging.\n",
        "  # log(history, start, precision, recall, fscore, acc_score, tn, fp, fn, tp)\n",
        "  min_val_loss = min(history.history['val_loss'])\n",
        "\n",
        "  now = datetime.now()\n",
        "  id = now.strftime('%Y%m%d%H%M%S')\n",
        "  elapsed_s = time.time() - start\n",
        "  elapsed = time.strftime('%H:%M:%S', time.gmtime(elapsed_s))\n",
        "\n",
        "  print(elapsed)\n",
        "\n",
        "  print('Accuracy: ', acc_score)\n",
        "  print('Precision: ', precision)\n",
        "  print('Recall: ', recall)\n",
        "  print('F-score: ', fscore)\n",
        "  '''\n",
        "  print('TN: ', tn)\n",
        "  print('FP: ', fp)\n",
        "  print('FN: ', fn)\n",
        "  print('TP: ', tp)\n",
        "  '''\n",
        "  return min_val_loss, precision, recall, fscore, acc, val_acc, loss, val_loss, id\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "oq8QorU4AeGY"
      },
      "outputs": [],
      "source": [
        "N = 1  # How many outer loops. Each contributes to the mean and standard deviation. 8\n",
        "M = 1  # Find the best (minimum) validation loss and fscore among M runs.4\n",
        "def main():\n",
        "  table_data = {}\n",
        "  for batch_size in BATCHES:\n",
        "            precisions = []\n",
        "            recalls = []\n",
        "            fscores = []\n",
        "            min_val_losses = []\n",
        "\n",
        "            # Evaluation framework.\n",
        "            for n in range(N):\n",
        "                best_precision = float('-inf')\n",
        "                best_recall = float('-inf')\n",
        "                best_fscore = float('-inf')\n",
        "                best_min_val_loss = float('inf')\n",
        "\n",
        "                # The learning algorithm.\n",
        "                # Choosing the best run based on training results among M runs.\n",
        "                for m in range(M):\n",
        "                    min_val_loss, precision, recall, fscore, acc, val_acc, loss, val_loss, id = run(batch_size)\n",
        "\n",
        "                    # Pick the best run based on the minimum validation loss.\n",
        "                    if min_val_loss < best_min_val_loss:\n",
        "                        best_min_val_loss = min_val_loss                \n",
        "                        best_fscore = fscore\n",
        "                        best_precision = precision\n",
        "                        best_recall = recall\n",
        "\n",
        "                    #plot(acc, val_acc, 'Accuracy', 'Validation accuracy', PLOT_SAVE_LOCATION, id, batch_size)\n",
        "                    #plot(loss, val_loss, 'Loss', 'Validation  loss', PLOT_SAVE_LOCATION, id, batch_size)\n",
        "\n",
        "                precisions.append(best_precision)\n",
        "                recalls.append(best_recall)\n",
        "                fscores.append(best_fscore)\n",
        "                min_val_losses.append(best_min_val_loss)\n",
        "\n",
        "            # Get results for LaTeX table.\n",
        "            p_mean = np.mean(precisions)\n",
        "            r_mean = np.mean(recalls)\n",
        "            f_mean = np.mean(fscores)\n",
        "            min_val_loss_mean = np.mean(min_val_losses)\n",
        "\n",
        "            p_std = np.std(precisions)\n",
        "            r_std = np.std(recalls)\n",
        "            f_std = np.std(fscores)\n",
        "            min_val_loss_std = np.std(min_val_losses)\n",
        "            '''\n",
        "            print(p_mean)\n",
        "            print(r_mean)\n",
        "            print(f_mean)\n",
        "            print(min_val_loss_mean)\n",
        "\n",
        "            print(p_std)\n",
        "            print(r_std)\n",
        "            print(f_std)\n",
        "            print(min_val_loss_std)\n",
        "            '''\n",
        "            table_data[batch_size] = {\n",
        "                'p_mean': p_mean,\n",
        "                'p_std': p_std,\n",
        "                'r_mean': r_mean,\n",
        "                'r_std': r_std,\n",
        "                'f_mean': f_mean,\n",
        "                'f_std': f_std\n",
        "            }\n",
        "\n",
        "    # Enable automatic LaTeX table creation of the results.\n",
        "    # create_latex_table(table_data, id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KnCTRbH5FGG6",
        "outputId": "22efd52e-a171-489d-e350-43a874f357cd"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-06-22 22:36:27.231143: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " block1_conv (Conv2D)        (None, 80, 12, 10)        220       \n",
            "                                                                 \n",
            " block1_pool (MaxPooling2D)  (None, 40, 6, 10)         0         \n",
            "                                                                 \n",
            " block1_norm (BatchNormaliza  (None, 40, 6, 10)        40        \n",
            " tion)                                                           \n",
            "                                                                 \n",
            " block2_conv (Conv2D)        (None, 40, 6, 20)         1820      \n",
            "                                                                 \n",
            " block2_pool (MaxPooling2D)  (None, 20, 3, 20)         0         \n",
            "                                                                 \n",
            " block2_norm (BatchNormaliza  (None, 20, 3, 20)        80        \n",
            " tion)                                                           \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 1200)              0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 256)               307456    \n",
            "                                                                 \n",
            " dense_norm (BatchNormalizat  (None, 256)              1024      \n",
            " ion)                                                            \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 256)               0         \n",
            "                                                                 \n",
            " pred (Dense)                (None, 2)                 514       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 311,154\n",
            "Trainable params: 310,582\n",
            "Non-trainable params: 572\n",
            "_________________________________________________________________\n",
            "Epoch 1/50\n",
            "1487/1487 [==============================] - 321s 211ms/step - loss: 0.0841 - acc: 0.9696 - val_loss: 0.0605 - val_acc: 0.9818\n",
            "Epoch 2/50\n",
            "1487/1487 [==============================] - 267s 180ms/step - loss: 0.0338 - acc: 0.9887 - val_loss: 0.0468 - val_acc: 0.9847\n",
            "Epoch 3/50\n",
            "1487/1487 [==============================] - 307s 206ms/step - loss: 0.0307 - acc: 0.9898 - val_loss: 0.0432 - val_acc: 0.9855\n",
            "Epoch 4/50\n",
            "1487/1487 [==============================] - 289s 194ms/step - loss: 0.0272 - acc: 0.9907 - val_loss: 0.0492 - val_acc: 0.9850\n",
            "Epoch 5/50\n",
            "1487/1487 [==============================] - 293s 197ms/step - loss: 0.0238 - acc: 0.9920 - val_loss: 0.0447 - val_acc: 0.9862\n",
            "Epoch 6/50\n",
            "1487/1487 [==============================] - 288s 193ms/step - loss: 0.0219 - acc: 0.9920 - val_loss: 0.0399 - val_acc: 0.9869\n",
            "Epoch 7/50\n",
            "1487/1487 [==============================] - 323s 217ms/step - loss: 0.0214 - acc: 0.9925 - val_loss: 0.0694 - val_acc: 0.9833\n",
            "Epoch 8/50\n",
            "1487/1487 [==============================] - 280s 188ms/step - loss: 0.0205 - acc: 0.9926 - val_loss: 0.0377 - val_acc: 0.9884\n",
            "Epoch 9/50\n",
            "1487/1487 [==============================] - 309s 208ms/step - loss: 0.0182 - acc: 0.9935 - val_loss: 0.0483 - val_acc: 0.9865\n",
            "Epoch 10/50\n",
            "1487/1487 [==============================] - 274s 184ms/step - loss: 0.0178 - acc: 0.9937 - val_loss: 0.0398 - val_acc: 0.9883\n",
            "Epoch 11/50\n",
            "1487/1487 [==============================] - 313s 211ms/step - loss: 0.0180 - acc: 0.9935 - val_loss: 0.0415 - val_acc: 0.9878\n",
            "Epoch 12/50\n",
            "1487/1487 [==============================] - 270s 182ms/step - loss: 0.0161 - acc: 0.9940 - val_loss: 0.0440 - val_acc: 0.9864\n",
            "Epoch 13/50\n",
            "1487/1487 [==============================] - 271s 182ms/step - loss: 0.0166 - acc: 0.9939 - val_loss: 0.0501 - val_acc: 0.9866\n",
            "Epoch 14/50\n",
            "1487/1487 [==============================] - 309s 208ms/step - loss: 0.0156 - acc: 0.9946 - val_loss: 0.0441 - val_acc: 0.9868\n",
            "Epoch 15/50\n",
            "1487/1487 [==============================] - 309s 208ms/step - loss: 0.0153 - acc: 0.9944 - val_loss: 0.0464 - val_acc: 0.9875\n",
            "Epoch 16/50\n",
            "1487/1487 [==============================] - 321s 216ms/step - loss: 0.0149 - acc: 0.9946 - val_loss: 0.0523 - val_acc: 0.9861\n",
            "Epoch 17/50\n",
            "1487/1487 [==============================] - 286s 192ms/step - loss: 0.0147 - acc: 0.9947 - val_loss: 0.0382 - val_acc: 0.9888\n",
            "Epoch 18/50\n",
            "1487/1487 [==============================] - 269s 181ms/step - loss: 0.0142 - acc: 0.9949 - val_loss: 0.0384 - val_acc: 0.9876\n",
            "Epoch 19/50\n",
            "1487/1487 [==============================] - 275s 185ms/step - loss: 0.0140 - acc: 0.9947 - val_loss: 0.0510 - val_acc: 0.9859\n",
            "Epoch 20/50\n",
            "1487/1487 [==============================] - 263s 177ms/step - loss: 0.0134 - acc: 0.9952 - val_loss: 0.0435 - val_acc: 0.9882\n",
            "Epoch 21/50\n",
            "1487/1487 [==============================] - 273s 184ms/step - loss: 0.0137 - acc: 0.9949 - val_loss: 0.0436 - val_acc: 0.9876\n",
            "Epoch 22/50\n",
            "1487/1487 [==============================] - 268s 180ms/step - loss: 0.0129 - acc: 0.9951 - val_loss: 0.0391 - val_acc: 0.9890\n",
            "Epoch 23/50\n",
            "1487/1487 [==============================] - 274s 184ms/step - loss: 0.0128 - acc: 0.9954 - val_loss: 0.0481 - val_acc: 0.9883\n",
            "Epoch 24/50\n",
            "1487/1487 [==============================] - 275s 185ms/step - loss: 0.0126 - acc: 0.9953 - val_loss: 0.0428 - val_acc: 0.9885\n",
            "Epoch 25/50\n",
            "1487/1487 [==============================] - 267s 180ms/step - loss: 0.0130 - acc: 0.9953 - val_loss: 0.0444 - val_acc: 0.9881\n",
            "Epoch 26/50\n",
            "1487/1487 [==============================] - 274s 184ms/step - loss: 0.0126 - acc: 0.9952 - val_loss: 0.0440 - val_acc: 0.9886\n",
            "Epoch 27/50\n",
            "1487/1487 [==============================] - 274s 185ms/step - loss: 0.0119 - acc: 0.9956 - val_loss: 0.0455 - val_acc: 0.9886\n",
            "Epoch 28/50\n",
            "1487/1487 [==============================] - 269s 181ms/step - loss: 0.0120 - acc: 0.9954 - val_loss: 0.0482 - val_acc: 0.9884\n",
            "Epoch 29/50\n",
            "1487/1487 [==============================] - 272s 183ms/step - loss: 0.0117 - acc: 0.9956 - val_loss: 0.0527 - val_acc: 0.9868\n",
            "Epoch 30/50\n",
            "1487/1487 [==============================] - 270s 182ms/step - loss: 0.0115 - acc: 0.9958 - val_loss: 0.0510 - val_acc: 0.9869\n",
            "Epoch 31/50\n",
            "1487/1487 [==============================] - 301s 202ms/step - loss: 0.0115 - acc: 0.9958 - val_loss: 0.0433 - val_acc: 0.9889\n",
            "Epoch 32/50\n",
            "1487/1487 [==============================] - 305s 205ms/step - loss: 0.0103 - acc: 0.9962 - val_loss: 0.0589 - val_acc: 0.9871\n",
            "Epoch 33/50\n",
            "1487/1487 [==============================] - 275s 185ms/step - loss: 0.0108 - acc: 0.9962 - val_loss: 0.0446 - val_acc: 0.9880\n",
            "Epoch 34/50\n",
            "1487/1487 [==============================] - 275s 185ms/step - loss: 0.0107 - acc: 0.9963 - val_loss: 0.0512 - val_acc: 0.9862\n",
            "Epoch 35/50\n",
            "1487/1487 [==============================] - 262s 176ms/step - loss: 0.0102 - acc: 0.9962 - val_loss: 0.0473 - val_acc: 0.9880\n",
            "Epoch 36/50\n",
            "1487/1487 [==============================] - 273s 183ms/step - loss: 0.0104 - acc: 0.9962 - val_loss: 0.0468 - val_acc: 0.9893\n",
            "Epoch 37/50\n",
            "1487/1487 [==============================] - 271s 183ms/step - loss: 0.0098 - acc: 0.9966 - val_loss: 0.0499 - val_acc: 0.9892\n",
            "Epoch 38/50\n",
            "1487/1487 [==============================] - 276s 186ms/step - loss: 0.0093 - acc: 0.9967 - val_loss: 0.0551 - val_acc: 0.9883\n",
            "Epoch 39/50\n",
            "1487/1487 [==============================] - 265s 178ms/step - loss: 0.0095 - acc: 0.9967 - val_loss: 0.0497 - val_acc: 0.9889\n",
            "Epoch 40/50\n",
            "1487/1487 [==============================] - 272s 183ms/step - loss: 0.0096 - acc: 0.9966 - val_loss: 0.0487 - val_acc: 0.9885\n",
            "Epoch 41/50\n",
            "1487/1487 [==============================] - 271s 182ms/step - loss: 0.0088 - acc: 0.9966 - val_loss: 0.0587 - val_acc: 0.9863\n",
            "Epoch 42/50\n",
            "1487/1487 [==============================] - 272s 183ms/step - loss: 0.0089 - acc: 0.9968 - val_loss: 0.0689 - val_acc: 0.9879\n",
            "Epoch 43/50\n",
            "1487/1487 [==============================] - 265s 178ms/step - loss: 0.0091 - acc: 0.9967 - val_loss: 0.0476 - val_acc: 0.9882\n",
            "Epoch 44/50\n",
            "1487/1487 [==============================] - 271s 182ms/step - loss: 0.0078 - acc: 0.9971 - val_loss: 0.0504 - val_acc: 0.9890\n",
            "Epoch 45/50\n",
            "1487/1487 [==============================] - 271s 182ms/step - loss: 0.0082 - acc: 0.9970 - val_loss: 0.0580 - val_acc: 0.9883\n",
            "Epoch 46/50\n",
            "1487/1487 [==============================] - 271s 183ms/step - loss: 0.0082 - acc: 0.9969 - val_loss: 0.0479 - val_acc: 0.9893\n",
            "Epoch 47/50\n",
            "1487/1487 [==============================] - 274s 184ms/step - loss: 0.0087 - acc: 0.9969 - val_loss: 0.0561 - val_acc: 0.9881\n",
            "Epoch 48/50\n",
            "1487/1487 [==============================] - 274s 185ms/step - loss: 0.0084 - acc: 0.9971 - val_loss: 0.0477 - val_acc: 0.9893\n",
            "Epoch 49/50\n",
            "1487/1487 [==============================] - 277s 186ms/step - loss: 0.0081 - acc: 0.9970 - val_loss: 0.0550 - val_acc: 0.9889\n",
            "Epoch 50/50\n",
            "1487/1487 [==============================] - 254s 171ms/step - loss: 0.0074 - acc: 0.9975 - val_loss: 0.0514 - val_acc: 0.9894\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-06-23 02:30:58.506229: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'inputs' with dtype float and shape [?,256]\n",
            "\t [[{{node inputs}}]]\n",
            "2023-06-23 02:30:58.882949: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'inputs' with dtype float and shape [?,256]\n",
            "\t [[{{node inputs}}]]\n",
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _update_step_xla while saving (showing 3 of 3). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: ./model_kd/assets\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: ./model_kd/assets\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "413/413 [==============================] - 4s 9ms/step\n",
            "03:54:37\n",
            "Accuracy:  0.9928857942934989\n",
            "Precision:  0.9483691521927825\n",
            "Recall:  0.9535898376753857\n",
            "F-score:  0.9509630342052442\n"
          ]
        }
      ],
      "source": [
        "main()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
